{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "yLjJCtPM0KBk",
        "Iwf50b-R2tYG",
        "ArJBuiUVfxKd",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarathklonappan/Unsupervised-Project/blob/main/Unsupervised_BBC_topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - TOPIC MODELLING ON NEWS ARTICLES\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 -**Sarath Lonappan\n",
        "##### **Team Member 2 -**Akash Raj\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains several articles belonging to diffrent topics.We must select most optimum number of topics and sort the articles to each topic.We have used count vectoriser to convert it to matrix and Latent Dirchlet Allocation algorithm to sort them.The articles have been allocated to respective topics after the modelling."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, the aim is to find the major topics across a collection of BBC news articles.Categorise each article to main topics using LDA."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall scikit-learn"
      ],
      "metadata": {
        "id": "IfeXFMB10_Y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==1.0.2"
      ],
      "metadata": {
        "id": "XAJU0Qlu10Mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing pyLDAvis to visualize the results of LDA model\n",
        "!pip install pyLDAvis==2.1.2"
      ],
      "metadata": {
        "id": "40yBwFdT1OUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# importing os module\n",
        "import os\n",
        "\n",
        "# importing CountVectorizer for feature extraction\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Importing data manipulation libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# importing tqdm and display modules for progress meters/bars\n",
        "from IPython.display import display\n",
        "from tqdm import tqdm\n",
        "\n",
        "# importing wordcloud to represent topics wordcloud\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Model selection modules\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Importing Counter\n",
        "from collections import Counter\n",
        "\n",
        "# import ast(abstract syntax tree)\n",
        "import ast\n",
        "\n",
        "# importing data visualization modules\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# importing mlab for implementing MATLAB functions\n",
        "import matplotlib.mlab as mlab\n",
        "\n",
        "# importing statistics module\n",
        "import scipy.stats as stats\n",
        "\n",
        "# importing TSNE for data exploration and visualizing high-dimensional data\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# importing decomposition modules\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "# importing Natural Language Toolkit and other NLP modules\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from textblob import TextBlob, Word\n",
        "\n",
        "# importing countVectorizer for text vectorization\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Importing warnings library. The warnings module handles warnings in Python.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "sklearn.__version__"
      ],
      "metadata": {
        "id": "E1gg4yqg0rzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "wm5eR1p9cihg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load Dataset\n",
        "\n",
        "# The variable \"directory\" holds the address of text files stored in drive\n",
        "path_dir = '/content/drive/MyDrive/M4/Unsupervised project'\n",
        "\n",
        "# All 5 sub-categories provided\n",
        "sub_dir = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
        "\n",
        "# Create dataframe for gathering the articles\n",
        "bbc_df = pd.DataFrame()\n",
        "\n",
        "# Iterate over sub-directories to access the text files\n",
        "for path in sub_dir:\n",
        "\n",
        "  # address to the subdirectory\n",
        "  dir = path_dir + '/' + path\n",
        "\n",
        "  # Iterate over all the text files present in a sub-directory\n",
        "  for filename in os.listdir(dir):\n",
        "\n",
        "    # Get file address\n",
        "    filepath = os.path.join(dir, filename)\n",
        "\n",
        "    # Traversing over text files and storing the articles into the dataframe\n",
        "    try:\n",
        "      data = open(filepath,'r').read()\n",
        "\n",
        "      # escape characters to be ignored in the text\n",
        "      escape = ['\\n']\n",
        "\n",
        "      # removing escape characters from text\n",
        "      for elem in escape:\n",
        "        data = data.replace(elem, ' ')\n",
        "\n",
        "      # Storing article to the dataframe\n",
        "      dict1 = {'Filename': filename.split('.')[0], 'Article': data.lower(), 'Category':path}\n",
        "      bbc_df = bbc_df.append(dict1, ignore_index=True, verify_integrity = True)\n",
        "\n",
        "    # Ignore exception, if any\n",
        "    except:\n",
        "      pass"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "bbc_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "bbc_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "bbc_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "bbc_df[bbc_df.duplicated(subset = \"Article\")]"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping duplicate entries\n",
        "bbc_df = bbc_df.drop_duplicates(subset=['Article'], keep='first')\n",
        "bbc_df"
      ],
      "metadata": {
        "id": "EW94kS0UezI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bbc_df.shape"
      ],
      "metadata": {
        "id": "kGvmodzhfc59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From info we found that no null values are present in this data frame"
      ],
      "metadata": {
        "id": "iQNBMDqFfCWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "bbc_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "bbc_df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame()"
      ],
      "metadata": {
        "id": "hmvcLxKciqqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Grouping rows based on categories and counting number of entries\n",
        "df=bbc_df.Category.value_counts()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "Ve_U3Bv2iWCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting a bar graph to represent all categories and number of articles in them\n",
        "df.plot(x = \"Category\", kind = \"bar\",\n",
        "        figsize=(8,5), grid = True,\n",
        "        xlabel = \"Categories\",\n",
        "        ylabel = \"Number of Articles\",\n",
        "        colormap= \"cividis\", width = 0.6,\n",
        "        rot = 45).patch.set_facecolor('#f0ffff')"
      ],
      "metadata": {
        "id": "5DvE6rF5haws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Articles**"
      ],
      "metadata": {
        "id": "zl12-itsY-FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reindexing the data and extracting just the contents of the article i.e, the complete transcript of the article\n",
        "\n",
        "# Reindexing the data\n",
        "reindexed_data = bbc_df.reset_index()\n",
        "\n",
        "# Extracting contents of all articles\n",
        "reindexed_data = reindexed_data[\"Article\"]\n",
        "\n",
        "# Display\n",
        "reindexed_data"
      ],
      "metadata": {
        "id": "KLH2mR36YbsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CountVectorizer (Top Words)**\n"
      ],
      "metadata": {
        "id": "XkHa47A1ZeRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract top n words with highest frequency\n",
        "def get_top_n_words(n_top_words, count_vectorizer, text_data):\n",
        "    '''\n",
        "    The function returns a tuple of the top n words in a sample and their\n",
        "    accompanying counts, given a CountVectorizer object and text sample as inputs\n",
        "    '''\n",
        "    # encoding the document using countvectorizer object\n",
        "    vectorized_content = count_vectorizer.fit_transform(text_data.values)\n",
        "    vectorized_total = np.sum(vectorized_content, axis=0)\n",
        "\n",
        "    # extracting specifics for words\n",
        "    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n",
        "    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n",
        "\n",
        "    # creating a vector matrix for words\n",
        "    word_vectors = np.zeros((n_top_words, vectorized_content.shape[1]))\n",
        "    for i in range(n_top_words):\n",
        "        word_vectors[i,word_indices[0,i]] = 1\n",
        "\n",
        "    # display Vector matrix\n",
        "    print(word_vectors)\n",
        "\n",
        "    # collect the words\n",
        "    words = [word[0].encode('ascii').decode('utf-8') for\n",
        "             word in count_vectorizer.inverse_transform(word_vectors)]\n",
        "\n",
        "    return (words, word_values[0,:n_top_words].tolist()[0])\n"
      ],
      "metadata": {
        "id": "f_pzBnAnZTZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing Stopwords using NLTK (Natural Language Toolkit)"
      ],
      "metadata": {
        "id": "y0NGUWFcZzVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dowloading nltk stopwords module\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "wcvWyGG1Zv_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting all stopwords for english language\n",
        "stpwrd = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# list of stopwords in english language\n",
        "stpwrd"
      ],
      "metadata": {
        "id": "zraqtfyxZ6jN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating vectorizer object\n",
        "count_vectorizer = CountVectorizer(stop_words=stpwrd)\n",
        "\n",
        "# calling the function to get words and their counts\n",
        "words, word_values = get_top_n_words(n_top_words=25,\n",
        "                                     count_vectorizer=count_vectorizer,\n",
        "                                     text_data=reindexed_data)\n",
        "\n",
        "# display top 25 words using bar plot\n",
        "fig, ax = plt.subplots(figsize=(16,8))\n",
        "ax.bar(range(len(words)), word_values, color=(0.1, 0.1, 0.1, 0.1),  edgecolor='blue')\n",
        "ax.set_xticks(range(len(words)))\n",
        "ax.set_xticklabels(words, rotation='vertical')\n",
        "ax.set_title('Top words in new articles dataset (excluding stop words)')\n",
        "ax.set_xlabel('Words')\n",
        "ax.set_ylabel('Number of occurences')\n",
        "ax.patch.set_facecolor('#f0ffff')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nIc-kfw4aCIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stemming/Lemmatizing the data**\n"
      ],
      "metadata": {
        "id": "O6yO50p9jFSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# contents of articles\n",
        "reindexed_data"
      ],
      "metadata": {
        "id": "AYXfryXnjFGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading wordNet lemmatizer\n",
        "nltk.download(['wordnet','omw-1.4'])\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "R2c5BZ_qaEmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatizing an article to see what wordnet returns\n",
        "for rows in reindexed_data:\n",
        " print(rows)\n",
        " test = [lemmatizer.lemmatize(words) for words in rows.split(' ')]\n",
        " print(test)\n",
        " break"
      ],
      "metadata": {
        "id": "bPXtQEopjSBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **lemmatization using Snowball Lemmatizer.**"
      ],
      "metadata": {
        "id": "0YHvQSvljaAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a lemmatizer object\n",
        "sno = nltk.stem.SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "BX6IM0V1jj2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatizing an article to see what snowball lemmatizer returns\n",
        "for rows in reindexed_data:\n",
        " print(rows)\n",
        " test = [sno.stem(words) for words in rows.split(' ')]\n",
        " print(test)\n",
        " break\n"
      ],
      "metadata": {
        "id": "mfv1R6dOjjsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **lemmatization using TextBlob.**"
      ],
      "metadata": {
        "id": "xOU8I3S2jsvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatizing using textblob\n",
        "for rows in reindexed_data:\n",
        " print(rows)\n",
        " test = [Word(words).lemmatize() for words in rows.split(' ')]\n",
        " print(test)\n",
        " break"
      ],
      "metadata": {
        "id": "qVsavzHGjudf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that Business and Sports category have the highest number of articles, while for Tech category the number is quite low. Number of articles will play a major role in determining the topics in them. Larger the number of articles for a category, higher will be the result satisfaction for the model."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Feature Extraction (Vectorization)***"
      ],
      "metadata": {
        "id": "xOAQkg8ukEuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a countvectorizer object\n",
        "count_vectorizer = CountVectorizer(stop_words = stpwrd, max_features = 4000)\n",
        "\n",
        "# text before vectorization\n",
        "text_sample = reindexed_data\n",
        "print('Content before vectorization: {}'.format(text_sample[121]))\n",
        "\n",
        "# encode the textual content\n",
        "document_term_matrix = count_vectorizer.fit_transform(text_sample)\n",
        "\n",
        "# text after vectorization\n",
        "print('Content after vectorization: \\n{}'.format(document_term_matrix[121]))\n"
      ],
      "metadata": {
        "id": "alxbq8jzkOgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML Model - 1\n",
        "Latent Dirichlet Allocation (LDA)"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameter tuning using Grid Search CV\n",
        "# Parameters tuning using Grid Search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "grid_params = {'n_components' : list(range(5,10))}\n",
        "\n",
        "# LDA model\n",
        "lda = LatentDirichletAllocation()\n",
        "lda_model = GridSearchCV(lda,param_grid=grid_params)\n",
        "lda_model.fit(document_term_matrix)\n",
        "\n",
        "# Best LDA model\n",
        "best_lda_model = lda_model.best_estimator_\n",
        "\n",
        "print(\"Best LDA model's params\" , lda_model.best_params_)\n",
        "print(\"Best log likelihood Score for the LDA model\",lda_model.best_score_)\n",
        "print(\"LDA model Perplexity on train data\", best_lda_model.perplexity(document_term_matrix))"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing pyLDAvis module\n",
        "import pyLDAvis.sklearn\n",
        "pyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "id": "QCmImfZEk1mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate and display the graph\n",
        "lda_panel = pyLDAvis.sklearn.prepare(best_lda_model, document_term_matrix, count_vectorizer, mds='tsne')\n",
        "lda_panel"
      ],
      "metadata": {
        "id": "3a-UQOJOk7Lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating docterms dataframe\n",
        "docterms = lda_panel.token_table.sort_values(by = ['Freq'], ascending=False)"
      ],
      "metadata": {
        "id": "myx8RuPZk9qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display docterms df\n",
        "docterms"
      ],
      "metadata": {
        "id": "oyYcHHIQlBm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Dataframe to contain top 50 terms by topic."
      ],
      "metadata": {
        "id": "p3iBohopmYFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create topics dataframe\n",
        "topicsdf = pd.DataFrame()\n",
        "# adding top 50 most relevant terms for each topic to the dataframe\n",
        "for i in range(1,6):\n",
        "  Topicdict ={ \"Topic\":i, \"Terms\":list(docterms[docterms['Topic']==i]['Term'].head(50))  }\n",
        "  topicsdf=topicsdf.append(Topicdict,ignore_index=True)\n",
        "topicsdf"
      ],
      "metadata": {
        "id": "GfN_zoRdmXlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Topic 1: Tech***"
      ],
      "metadata": {
        "id": "TlcH9IlJmuvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating term freq dict for topic 1\n",
        "t1dict = {}\n",
        "for vals in docterms[docterms['Topic']==1].head(40).values:\n",
        "  t1dict[vals[2]] =vals[1]\n",
        "t1dict"
      ],
      "metadata": {
        "id": "_jY7XO8Rm0TZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating the Wordcloud for topic 1\n",
        "wordcloud = WordCloud(width = 1200, height = 700,\n",
        "                background_color ='beige',\n",
        "                min_font_size = 10).generate(' '.join(list(t1dict.keys())))\n",
        "wordcloud = wordcloud.generate_from_frequencies(frequencies=t1dict)"
      ],
      "metadata": {
        "id": "5Woykqkem4-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the WordCloud image\n",
        "plt.figure(figsize = (12,7), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O5AFqQ59m-bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic 2: **Business**"
      ],
      "metadata": {
        "id": "T7UNyJK8nCZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating term freq dict for topic 2\n",
        "t2dict = {}\n",
        "for vals in docterms[docterms['Topic']==2].head(40).values:\n",
        "  t2dict[vals[2]] =vals[1]\n",
        "t2dict"
      ],
      "metadata": {
        "id": "sGNU36dRnGvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the wordcloud for topic 2\n",
        "wordcloud = WordCloud(width = 1200, height = 700,\n",
        "                background_color ='black',\n",
        "                min_font_size = 10).generate(' '.join(list(t2dict.keys())))\n",
        "wordcloud = wordcloud.generate_from_frequencies(frequencies=t2dict)"
      ],
      "metadata": {
        "id": "lYvqrgwEnZNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the WordCloud image\n",
        "plt.figure(figsize = (12,7), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e1oHH8FPnZ5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Topic 3: Sports**"
      ],
      "metadata": {
        "id": "ZxOW0ZAJnfBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating term freq dict for topic 3\n",
        "t3dict = {}\n",
        "for vals in docterms[docterms['Topic']==3].head(40).values:\n",
        "  t3dict[vals[2]] =vals[1]\n",
        "t3dict"
      ],
      "metadata": {
        "id": "HMtBF8T8netx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the Wordcloud for topic 3\n",
        "wordcloud = WordCloud(width = 1200, height = 700,\n",
        "                background_color ='magenta',\n",
        "                min_font_size = 10).generate(' '.join(list(t3dict.keys())))\n",
        "wordcloud = wordcloud.generate_from_frequencies(frequencies=t3dict)"
      ],
      "metadata": {
        "id": "PRdcSzwXnnbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the WordCloud image\n",
        "plt.figure(figsize = (12,7), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1f-czpIjnnWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Topic 4: Politics***"
      ],
      "metadata": {
        "id": "28v6FUqXnvwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating term freq dict for topic 4\n",
        "t4dict = {}\n",
        "for vals in docterms[docterms['Topic']==4].head(40).values:\n",
        "  t4dict[vals[2]] =vals[1]\n",
        "t4dict"
      ],
      "metadata": {
        "id": "kgMbc6Z6nwIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the wordcloud for topic 4\n",
        "wordcloud = WordCloud(width = 1200, height = 700,\n",
        "                background_color ='cyan',\n",
        "                min_font_size = 10).generate(' '.join(list(t4dict.keys())))\n",
        "wordcloud = wordcloud.generate_from_frequencies(frequencies=t4dict)"
      ],
      "metadata": {
        "id": "_e76q3ZCnw8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the WordCloud image\n",
        "plt.figure(figsize = (12,7), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aWxeD7vtnwmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Topic 5: Entertainment***"
      ],
      "metadata": {
        "id": "7aqGu21AnxWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating term freq dict for topic 5\n",
        "t5dict = {}\n",
        "for vals in docterms[docterms['Topic']==5].head(40).values:\n",
        "  t5dict[vals[2]] =vals[1]\n",
        "t5dict"
      ],
      "metadata": {
        "id": "HtBq7h4Cnx8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the wordcloud for topic 5\n",
        "wordcloud = WordCloud(width = 1200, height = 700,\n",
        "                background_color ='maroon',\n",
        "                min_font_size = 10).generate(' '.join(list(t5dict.keys())))\n",
        "wordcloud = wordcloud.generate_from_frequencies(frequencies=t5dict)"
      ],
      "metadata": {
        "id": "-Dw0sPYHnybY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the WordCloud image\n",
        "plt.figure(figsize = (12,7), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fCOpnHYwnyRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q2kWg1P6opdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "miCk3M3Aoo2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}